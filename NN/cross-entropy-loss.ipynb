{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4384d67-50f8-454f-a4fe-b381b2270d54",
   "metadata": {},
   "source": [
    "---\n",
    "#### **Loss Function**\n",
    "---\n",
    "- The ***loss function***, also the ***cost function***, is the algorithm that quantifies how wrong a model is.\n",
    "- Loss is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0.\n",
    "- Mean squared error or Squared error is used in regression but in classification we will use ***crossentropy***\n",
    "- ***Categorical cross-entropy*** is explicitly used to compare a “ground-truth” probability (y or “targets”) and some predicted distribution (y-hat or “predictions”)\n",
    "- It is also one of the most commonly used loss functions with a softmax activation on the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57be56-e5b3-4503-96ba-081bb8d51046",
   "metadata": {},
   "source": [
    "$$L_{i} = -\\sum_{j}y_{i,j}\\log(\\hat{y}_{i,j})\\tag{1}$$\n",
    "\n",
    "- Where **$L_{i}$** denotes sample loss value, **$i$** is the i-th sample in the set, $j$ is the label/output index, $y$ denotes the target values, and $\\hat{y}$ denotes the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda9a16-130d-4a90-a6c5-1c3807887c4f",
   "metadata": {},
   "source": [
    "$$L_{i} = -\\log(\\hat{y}_{i,k})\\tag{2}$$\n",
    "- Where **$L_{i}$** denotes sample loss value, **$i$** is the i-th sample in a set, $k$ is the index of the target label (ground-true label), $y$ denotes the target values and $\\hat{y}$ denotes the predicted values.\n",
    "- **NB**: targets are one hot encoded values & They can also be sparse, which means that the numbers they contain are the correct class numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e124d19-6c63-4f1d-9319-5c49f953df04",
   "metadata": {},
   "source": [
    "---\n",
    "##### **Categorical Cross-entropy in single training**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8e24c2-a722-405e-98e0-e7e0c1c45d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "loss = -(math.log(softmax_output[0])*target_output[0]+\n",
    "math.log(softmax_output[1])*target_output[1] +\n",
    "math.log(softmax_output[2])*target_output[2])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c13e22-0f47-4726-bfd7-2e5c49a3824b",
   "metadata": {},
   "source": [
    "---\n",
    "##### **Categorical Cross-entropy in multiple training samples**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c6ad56-f67b-48aa-91ff-5cbd8581b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = [[0.7, 0.1, 0.2], # softmax outputs of three samples \n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]]\n",
    "class_targets = [0, 1, 1] # target class for cat=1, dog=0 and human=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15d63c4-c736-43a1-82bc-2724e6f61d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "for idx, soft in  zip(class_targets, softmax_outputs):\n",
    "    print(soft[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4ff8b-1f2b-4246-bf26-5195b57b8f04",
   "metadata": {},
   "source": [
    "---\n",
    "- Class target at 0 indicates the softmax output with greater indices of 0.7\n",
    "- Class target at both of 1 indicates the softmax output with greater indices of 0.5 and 0.9 in both cases\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8cc5cd-2aa0-4a40-b9a3-4901d122310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], # softmax outputs of three samples \n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]])\n",
    "print(softmax_outputs[[0,1,1], class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eadfa20-3c22-4422-a8c6-928a78738e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(softmax_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ee17425-7b33-4a1d-8ed0-521ea20c98b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[range(len(softmax_outputs)),class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61bc34af-c59e-4ae6-adba-7c79bbf7f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)),class_targets])) # entire loss for training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5163b323-1d74-4800-ab5d-53330db5be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# to find the average loss per batch\n",
    "losses = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "average_loss = np.mean(losses)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb21218e-f331-4d70-a486-627ccf416191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if the target output is spiral\n",
    "class_targets  = np.array([[ 1,  0, 0],\n",
    "                            [0, 1, 0],\n",
    "                            [0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0a8b7ff-5b76-4735-ad61-dd22d184bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(class_targets.shape) == 1:\n",
    "    confidences = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "elif len(class_targets.shape) == 2:\n",
    "    confidences = np.sum(softmax_outputs*class_targets, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6afe2622-0e13-4a99-9119-1414e88f226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "lg = -np.log(confidences)\n",
    "print(np.mean(lg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e0004-b543-489f-87d9-957ad5d2330c",
   "metadata": {},
   "source": [
    "---\n",
    "- The softmax consists of numbers in the range from 0 to 1 - a list of confidences.\n",
    "- It is possible that the model will have full confidence for one label making all the remaining confidences zero. \n",
    "- Similarly, it is also possible that the model will assign full confidence to a value that wasn’t the target.\n",
    "---\n",
    "- What if the ***np.log(0)***?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b24b3720-7b70-4b20-a289-becd65267842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_loss = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "738231c9-1e13-4b93-b36c-5227e64eac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        #calculate sample\n",
    "        sample = len(y_pred)\n",
    "        #clip the y to prevent division by 0\n",
    "        y_pred_clip = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        if len(y_pred_clip.shape) == 1:\n",
    "            confidence = y_pred_clip[range(sample), y_true]\n",
    "        elif len(y_pred_clip.shape) == 2:\n",
    "            confidence = np.sum(y_true*y_pred_clip, axis = 1)\n",
    "        negLL = -np.log(confidence)\n",
    "        return negLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "637954d2-fcfe-4dd2-86ba-b4571756d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss = CrossEntropy()\n",
    "los = loss.calculate(softmax_outputs, class_targets)\n",
    "print(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcc288-6b2c-42b4-ae6e-f4af4ef30e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
